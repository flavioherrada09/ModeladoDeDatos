{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://p0.pikrepo.com/preview/226/186/artificial-intelligence-concept.jpg\" width=\"350px\" height=\"180px\" />\n",
    "\n",
    "\n",
    "# <font color= #8A0829> Laboratorio de Modelado de Datos </font>\n",
    "- <Strong> Flavio Maximiliano Herrada Avalos</Strong>\n",
    "- <Strong> Año </Strong>: 2021\n",
    "- <Strong> Email: </Strong>  <font color=\"blue\"> `flavio.herrada@iteso.mx` </font>\n",
    "___\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://p0.pikrepo.com/preview/226/186/artificial-intelligence-concept.jpg.</p>\n",
    "\n",
    "### <font color= #2E9AFE> Tema: Procesamiento de Datos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datos: Glass Identification Data Set\n",
    "Los datos se pueden encontrar en:\n",
    "https://archive.ics.uci.edu/ml/datasets/glass+identification\n",
    "\n",
    "Se busca identificar qué tipo de vidrio es una muestra. \n",
    "\n",
    "\n",
    "- 1. Id number: 1 a 214\n",
    "- 2. RI: Indice Refractivo(medida para saber cuánto se reduce la velocidad de la luz al atravesarlo)\n",
    "- 3. Na: Sodio (unidad de medida: porcentaje en peso en el óxido correspondiente, como son los atributos 4-10)\n",
    "- 4. Mg: Magnesio\n",
    "- 5. Al: Aluminio\n",
    "- 6. Si: Silicon\n",
    "- 7. K: Potasio\n",
    "- 8. Ca: Calcio\n",
    "- 9. Ba: Bario\n",
    "- 10. Fe: Hierro\n",
    "- 11. Tipo de Vidrio: \n",
    "-- 1 ventanas de edificios procesadas por flotación \n",
    "-- 2 ventanas de edificios no procesadas por flotación \n",
    "-- 3 ventanas de vehículos procesadas por flotación\n",
    "-- 4 ventanas de vehículos no procesadas por flotación \n",
    "-- 5 contenedores\n",
    "-- 6 vajilla\n",
    "-- 7 faros\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://archive.ics.uci.edu/ml/assets/logo.gif\" width=\"350px\" height=\"180px\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías\n",
    "\n",
    "\n",
    "Las librerías son un conjunto de módulos y paquetes donde muchas de las operaciones más comúnes de la programación diaria ya están implementadas en ellas, de modo que podemos concentrarnos en lo que realmente nos ocupa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar Librerías\n",
    "import numpy as np #para trabajar con arreglos numéricos\n",
    "import pandas as pd #para trabajar con dataframes\n",
    "import matplotlib.pyplot as plt #visualizaciones\n",
    "import seaborn as sns #visualizaciones\n",
    "from sklearn import preprocessing #librería de machine learning en python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más información sobre qué es una librería:\n",
    "\n",
    "https://immune.institute/blog/librerias-python-que-son/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar datos\n",
    "\n",
    "#los archivos \".data\" los podemos leer con la función read_csv pero es necesario decirle cómo se llaman nuestras variables\n",
    "data = pd.read_csv('glass.data',header=None)\n",
    "names = ['ID','Indice_Refraccion','Na','Mg','Al','Si','K', 'Ca','Ba','Fe','Tipo_Vidrio']\n",
    "data.columns = names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¿Qué es data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimension de los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vistazo de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tipo de cada dato\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tipos de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valores nulos de una columna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valores nulos\n",
    "missing = pd.DataFrame(data.isnull().sum(),columns=['Valores_Nulos'])\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo bien! no hay valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valores unicos de una columna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valores únicos\n",
    "unival = pd.DataFrame(data.nunique(), columns=['Valores_Unicos'])\n",
    "unival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detalles estadísticos\n",
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tipo</th>\n",
       "      <th>Valores_Nulos</th>\n",
       "      <th>Valores_Unicos</th>\n",
       "      <th>Max</th>\n",
       "      <th>Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>214</td>\n",
       "      <td>214.00000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indice_Refraccion</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>1.53393</td>\n",
       "      <td>1.51115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Na</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>17.38000</td>\n",
       "      <td>10.73000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mg</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>4.49000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Al</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>0.29000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Si</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>75.41000</td>\n",
       "      <td>69.81000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>6.21000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ca</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>143</td>\n",
       "      <td>16.19000</td>\n",
       "      <td>5.43000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ba</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>3.15000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fe</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.51000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tipo_Vidrio</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Tipo  Valores_Nulos  Valores_Unicos        Max       Min\n",
       "ID                   int64              0             214  214.00000   1.00000\n",
       "Indice_Refraccion  float64              0             178    1.53393   1.51115\n",
       "Na                 float64              0             142   17.38000  10.73000\n",
       "Mg                 float64              0              94    4.49000   0.00000\n",
       "Al                 float64              0             118    3.50000   0.29000\n",
       "Si                 float64              0             133   75.41000  69.81000\n",
       "K                  float64              0              65    6.21000   0.00000\n",
       "Ca                 float64              0             143   16.19000   5.43000\n",
       "Ba                 float64              0              34    3.15000   0.00000\n",
       "Fe                 float64              0              32    0.51000   0.00000\n",
       "Tipo_Vidrio          int64              0               6    7.00000   1.00000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualizar todo junto\n",
    "def reporte(datos):\n",
    "    dtyp = pd.DataFrame(datos.dtypes, columns=['Tipo'])\n",
    "    missing = pd.DataFrame(datos.isnull().sum(), columns=['Valores_Nulos'])\n",
    "    unival = pd.DataFrame(data.nunique(), columns=['Valores_Unicos'])\n",
    "    maximo = pd.DataFrame(data.max(), columns=['Max'])\n",
    "    minimo = pd.DataFrame(data.min(), columns=['Min'])\n",
    "    return dtyp.join(missing).join(unival).join(maximo).join(minimo)\n",
    "reporte(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿En verdad son todas las variables numéricas?\n",
    "<font color= #8A0829>  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficar variables\n",
    "plt.scatter(data.Tipo_Vidrio, data.Indice_Refraccion)\n",
    "plt.xlabel('Tipo Vidrio'),plt.ylabel('Indice de Refraccion')\n",
    "plt.title(\"Indice Refracion vs Tipo Vidrio\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuántos datos hay de cada tipo de vidrio\n",
    "data['Tipo_Vidrio'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficar una sola variable\n",
    "plt.scatter(data.ID,data.Indice_Refraccion)\n",
    "plt.xlabel('ID'),plt.ylabel('Indice de Refraccion')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficar una varible con otra\n",
    "plt.scatter(data.K,data.Ca)\n",
    "plt.xlabel('K'),plt.ylabel('Ca')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficar una varible con otra y colorear por tipo de vidrio\n",
    "sns.lmplot(x='K', y='Ca', data=data, hue='Tipo_Vidrio', fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centrado y Escalamiento\n",
    "*¿Qué es?*: Es poner valores en el mismo rango o escala para que ninguna variable sea dominada por otra\n",
    "\n",
    "*¿Porqué sirve?*: La mayoría de las veces, los datos tienen variables que varían mucho en magnitudes, unidades y rango. Pero dado que la mayoría de los algoritmos de aprendizaje automático utilizan la distancia euclidiana entre dos puntos de datos en sus cálculos, esto es un problema.\n",
    "\n",
    "Las variables con magnitudes altas pesarán mucho más en los cálculos de distancia que las variables con magnitudes bajas. Para que esto no afecte tanto, debemos llevar todas las variables al mismo nivel de magnitudes. Esto se puede lograr escalando.\n",
    "<br>\n",
    "\n",
    "Algunos algoritmos donde el escalamiento importa:\n",
    "- KNN: con la ditancia Euclidiana es sensible a magnitudes\n",
    "- PCA: este busca traer características con máxima varianza y la varianza es grande para variables con grandes magnitudes. Esto sesga el PCA a variables con grandes magnitudes \n",
    "- Gradiente Descendiente: se puede acelerar escalando. Ya que $\\theta$ desciende más rápido en rangos pequeños y eso hace que oscile menos ya que los brincos son más parejos\n",
    "\n",
    " <br>\n",
    " \n",
    "<img style=\"float: center; margin: 0px 0px 15px 15px;\" src=\"https://miro.medium.com/max/2000/1*yR54MSI1jjnf2QeGtt57PA.png\" width=\"350px\" height=\"180px\" />\n",
    "<br>\n",
    "Observando los datos originales, vemos que cada variable tiene diferentes escalas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos originales\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Escalar datos**\n",
    "\n",
    "*Estandarización*\n",
    "\n",
    "La ecuación para estandarizar datos (MinMaxScaler):\n",
    "\n",
    "$$X\\prime = \\frac{X-X_{min}}{X_{max}-X_{min}}$$\n",
    "\n",
    "Los valores quedan entre un rango específico como \\[0,1\\]. Trata de hacer que los valores sean cercanos a su media\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a mano para Indice de Refracción\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con librería\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "names = data.columns\n",
    "d = scaler.fit_transform(data)\n",
    "scaled_df = pd.DataFrame(d, columns=names)\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficar nuevas variables\n",
    "sns.lmplot(x='K', y='Ca', data=scaled_df, hue='Tipo_Vidrio', fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos variable original (Ca) vs variable escalada por minmax\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(data.Ca, ax=ax[0], color='y')\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(scaled_df.Ca, ax=ax[1])\n",
    "ax[1].set_title(\"Scaled data\")\n",
    "plt.show()\n",
    "#La distribución no cambia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Normalización*\n",
    "\n",
    "La ecuación para escalar las variables por normalización (z-score normalization):\n",
    "\n",
    "$$x\\prime=\\frac{x-\\bar x}{\\sigma}$$\n",
    "\n",
    "Transforma los datos tal que la distribución resultante tiene media de 0 y desviación estándar de 1.\n",
    "Cuando hay datos atípicos que son importantes y no queremos perder su impacto, usamos esta normalización. Aquí sí se obtienen datos negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a mano para indice de refracción normalizacion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con librería\n",
    "names = data.columns\n",
    "d = preprocessing.scale(data)\n",
    "standard = pd.DataFrame(d, columns=names)\n",
    "standard.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos variable original (Ca) vs variable escalada por z-score\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(data.Ca, ax=ax[0], color='y')\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(standard.Ca, ax=ax[1])\n",
    "ax[1].set_title(\"Scaled data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuál es la diferencia entre estandarizar y normalizar?**\n",
    "\n",
    "Escalar, está cambiando el rango de los datos mientras que en la normalización está cambiando la forma de distribución de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asimetría en los datos\n",
    "\n",
    "La probabilidad de que los datos aparezcan por arriba de su media no es la misma que para valores por debajo de la media. \n",
    "\n",
    "Si estamos seguros que los datos son correctos, se puede realizar una transformación para evitar la asimetría\n",
    "\n",
    "<img style=\"float: center; margin: 0px 0px 15px 15px;\" src=\"https://miro.medium.com/max/765/1*hxVvqttoCSkUT2_R1zA0Tg.gif\" width=\"450px\" height=\"280px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogramas\n",
    "data.hist(bins=30,figsize=(20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Efectos de la asimetría\n",
    "En datos asimétricos, la región de la cola puede actuar como un valor atípico para el modelo estadístico y sabemos que los valores atípicos afectan negativamente el rendimiento del modelo, especialmente los modelos basados en regresión. Hay modelos estadísticos que son robustos a valores atípicos como los modelos basados en árboles, pero limitarán la posibilidad de probar otros modelos. Por lo tanto, es necesario transformar los datos asimétricos para que se acerquen lo suficiente a una distribución gaussiana o distribución normal. Esto nos permitirá probar más modelos estadísticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regla empírica basada en el cálculo de la relación entre el valor máximo y el mínimo de una variable:\n",
    "$$\\frac{max(x_{i})}{min({x_{i}})}>20$$\n",
    "\n",
    "Fórmula del índice de asimetría estática:\n",
    "$$skewness = \\frac{\\sum_{i=1}^{m}(x_{i}-\\bar x_{i})^{3}}{(m-1)v^{3/2}}, v=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar x_{i})^{2}}{m-1} $$\n",
    "\n",
    "Podemos determinar el nivel de asimetría de la forma:\n",
    "- Distribución altamente sesgada: si el valor del sesgo es menor a -1 o mayor a +1.\n",
    "- Distrubición moderadamente sesgada: si el valor del sesgo es entre -1 y -0.5 o entre +0.5 y +1\n",
    "- Distribución aproximadamente simétrica: si el valor del sesgo está entre -0.5 y +0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sacar a mano la asimetría\n",
    "def skewness(data):\n",
    "    v = np.sum(np.power(data-data.mean(axis=0),2))/(data.shape[0]-1)\n",
    "    skewness=np.sum(np.power(data-data.mean(axis=0),3))/((data.shape[0]-1)*np.power(v,3/2))\n",
    "    return(skewness)\n",
    "skewness(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo de la asimetria con pandas\n",
    "skewness = round(data.skew(),2)\n",
    "skewness = skewness.to_frame()\n",
    "skewness = skewness.rename(columns={0: \"value\"}) \n",
    "\n",
    "def f(x):\n",
    "    if x['value'] < -1 or x['value'] > 1: return 'Highly Skewed'\n",
    "    elif (x['value']<=0 and x['value']>=-0.5) or (x['value'] >=0 and x['value']<=0.5):\n",
    "        return 'Symmetric distribution'\n",
    "    else: return 'Moderately skewed'\n",
    "    \n",
    "skewness['skewness'] = skewness.apply(f, axis=1)\n",
    "\n",
    "skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ¿Cómo mitigar la asimetría?\n",
    "\n",
    "_Box Cox_: Una transformación de Box Cox es una transformación de variables no normales en una forma normal. (Lambda entre -5 y 5). Aplicada a valores positivos. \n",
    "\n",
    "$$y(\\lambda)=\n",
    "    \\begin{cases}\n",
    "      \\frac{(y)^{\\lambda}-1}{\\lambda} & \\text{if $\\lambda\\neq0$}\\\\\n",
    "      \\log(y) & \\text{if $\\lambda=0$ }\\\\\n",
    "     \\end{cases}$$\n",
    "\n",
    "\n",
    "**Datos con sesgo positivo**\n",
    "\n",
    "_Transformación de raíz cúbica:_ Es una transformación bastante fuerte con un efecto sustancial en la forma de la distribución, pero es más débil que el logaritmo. También se puede aplicar a valores negativos y cero.\n",
    "$$x_{i}^{*}=(x_{i})^{1/3}$$\n",
    "\n",
    "_Transformación de raíz cuadrada:_ Aplicado solo a valores positivos. \n",
    "$$x_{i}^{*}=\\sqrt(x_{i})$$\n",
    "\n",
    "_Transformación logarítmica:_ Es una transformación fuerte que puede ser usada para reducir el sesgo positivo. Aplicado solo a valores positivos\n",
    "$$x_{i}^{*}=\\log(x_{i})$$\n",
    "\n",
    "**Datos con sesgo negativo**\n",
    "\n",
    "Las transformaciones comunes incluyen elevar al cuadrado, raíz cúbica y logarítmica.\n",
    "\n",
    "_Transformación al cuadrado:_ Tiene un efecto moderado en la forma de la distribución y podría usarse para reducir el sesgo hacia la izquierda.\n",
    "$$x_{i}^{*}=(x_{i})^{2}$$\n",
    "\n",
    "\n",
    "Otro método para manejar la asimetría es encontrar valores atípicos y posiblemente eliminarlos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varias transformaciones\n",
    "data1=data.copy()\n",
    "#Logarítmica\n",
    "data1['Indice_Refraccion_no_skewness'] = np.log(data1.Indice_Refraccion)\n",
    "#Elevar al cuadrado\n",
    "data1['Indice_Refraccion_no_skewness'] = np.sqrt(data1.Indice_Refraccion)\n",
    "\n",
    "## Transformacion BoxCox usando scipy\n",
    "from scipy import stats\n",
    "data1['Indice_Refraccion_no_skewness'] = stats.boxcox(data1.Indice_Refraccion,lmbda=-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparación de datos sesgados y transformados\n",
    "fig = plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(data1.Indice_Refraccion)\n",
    "plt.xlabel('Indice_Refraccion'),plt.ylabel('Frecuencia')\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(data1.Indice_Refraccion_no_skewness)\n",
    "plt.xlabel('Indice_Refraccion_no_skewness'),plt.ylabel('Frecuencia')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sesgo para el índice de refracción transformado\n",
    "round(data1.Indice_Refraccion_no_skewness.skew(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores Atípicos\n",
    "\n",
    "Un dato se considera atípico si aparece por fuera de un rango que se considera válido o\n",
    "aceptable para el fenómeno que se analiza.\n",
    "\n",
    "Para determinar si los datos tienen valores atípicos es posible usar gráficas de bigotes en los\n",
    "datos.\n",
    "\n",
    "Para cuantificar el rango aceptable o permitido para encontrar datos atípicos se utiliza el\n",
    "parámetro IQR (inter quantilerange).\n",
    "\n",
    "$$IQR=75^{th}quantile - 25^{th}quantile$$\n",
    "$$upper_{boundary}=75^{th}quantile+(\\alpha*IQR)$$\n",
    "$$lower_{boundary}=25^{th}quantile-(\\alpha*IQR)$$\n",
    "\n",
    "donde $\\alpha$=1.5, y está relacionado a la probabilidad de ocurrencia si los datos tuvieran una distribución normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot de varias las variables\n",
    "ax = sns.boxplot(data=data.iloc[:,6:9], orient=\"h\", palette=\"Set2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un boxplot no se ve afectado por el escalamiento\n",
    "#Datos sin transformar\n",
    "sns.boxplot(y=data['Indice_Refraccion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos escalados\n",
    "sns.boxplot(y=standard['Indice_Refraccion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para determinar outliers\n",
    "def find_boundaries(df_var,distance=1.5):\n",
    "    IQR = df_var.quantile(0.75)-df_var.quantile(0.25)\n",
    "    lower = df_var.quantile(0.25)-IQR*distance\n",
    "    upper = df_var.quantile(0.75)+IQR*distance\n",
    "    return lower,upper\n",
    "\n",
    "lmin,lmax = find_boundaries(data['Indice_Refraccion'])\n",
    "outliers = np.where(data['Indice_Refraccion'] > lmax, True,np.where(data['Indice_Refraccion'] < lmin, True, False))\n",
    "outliers_df = data.loc[outliers, 'Indice_Refraccion']\n",
    "outliers_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tranformación de datos atípicos\n",
    "Si los datos atípicos son importantes y no es posible descartarlos, entonces \n",
    "es posible hacer una transformación de los datos para que se encuentren en un espacio con escalas similares.\n",
    "\n",
    "Una transformación que se puede hacer es utilizar una función que mitigue la asimetría. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Escalar o quitar datos atípicos primero?**\n",
    "\n",
    "Depende.... pero se recomienda primero quitar los outliers y luego escalar los datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos Nulos\n",
    "\n",
    "Un dato nulo se refiere a la ausencia de un valor o valores en al menos una de las variables.\n",
    "El origen de estos valores pueden ser distintos (errores de captura, censura, funcionamiento del fenómeno, etc.), pero hay que tratar de identificarlas para poder realizar el tratamiento correcto.\n",
    "\n",
    "Si los datos no pueden ser eliminados y es necesario sustituirlos para poder procesarlos con algoritmos sensibles a la falta de datos.\n",
    "Dentro de los criterios comúnmente usados para sustituir los valores nulos se encuentran.\n",
    "\n",
    "◦ Sustitución por una constante\n",
    "\n",
    "◦ Sustitución por una etiqueta\n",
    "\n",
    "◦ Sustitución por la media\n",
    "\n",
    "◦ Sustitución por la mediana\n",
    "\n",
    "◦ Sustitución por el más frecuente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insertar datos nulos (10%)\n",
    "data_nulos = data.mask(np.random.random(data.shape) < .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contar datos nulos en nuevo dataset\n",
    "pd.DataFrame(data_nulos.isnull().sum(), columns=['Valores_Nulos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputar datos nulos\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=99)\n",
    "imputer = SimpleImputer(strategy='constant', fill_value='Missing')\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "imputer.fit(data_nulos)\n",
    "imputer.statistics_ # revisar los valores por los que remplazará\n",
    "data_nulos_imputados = imputer.transform(data_nulos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nulos_imputados = pd.DataFrame(data_nulos_imputados, columns=data.columns)\n",
    "pd.DataFrame(data_nulos_imputados.isnull().sum(), columns=['Valores_Nulos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos Imbalanceados\n",
    "\n",
    "El problema de los datos imbalanceados es que al predecir datos se tiene una gran exactitud prediciendo la clase mayoritaria, pero falla en capturar la clase minoritaria, que suele ser el objetivo de crear el modelo en primer lugar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar datos\n",
    "fraud = pd.read_csv('fraud_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¿Cuántos datos son fraude?\n",
    "fraud.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficar fraude vs no fraude\n",
    "g = sns.countplot(fraud['Class'])\n",
    "g.set_xticklabels(['Not Fraud', 'Fraud'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué pasa si hago predicciones con los datos así como están?\n",
    "\n",
    "- Para todas esas transacciones que no son fraude, se va a tener una exactitud del 100%\n",
    "- Para esas transacciones que son fraudalentas, se tendría una exactitud del 0%\n",
    "- La exactitud en general sería alta simplemente porque la mayoría de las transacciones no son fraudulentas (no porque su modelo sea bueno).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Técnicas de remuestreo\n",
    "\n",
    "Consiste en eliminar muestras de la clase mayoritaria (submuestreo) y / o agregar más ejemplos de la clase minoritaria (sobremuestreo).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='resampling.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Haciendo una Regresión Logística con datos imbalanceados\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x = fraud.iloc[:, :-1]\n",
    "y = fraud.iloc[:, -1]\n",
    "\n",
    "#Creación de la regresión \n",
    "model =  LogisticRegression()\n",
    "model.fit(x, y)\n",
    "y_predict = model.predict(x)\n",
    "accuracy_score(y_predict, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conteo de clases\n",
    "class_count_0, class_count_1 = fraud['Class'].value_counts()\n",
    "\n",
    "#separamos las clases\n",
    "class_0 = fraud[fraud['Class'] == 0]\n",
    "class_1 = fraud[fraud['Class'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submuestreo\n",
    "class_0_under = class_0.sample(class_count_1)\n",
    "\n",
    "test_under = pd.concat([class_0_under, class_1], axis=0)\n",
    "\n",
    "print(\"Total de clase 1 y 0:\",test_under['Class'].value_counts())#grafica la cuenta después del submuestreo\n",
    "test_under['Class'].value_counts().plot(kind='bar', title='Cuenta (Class)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sobremuestreo\n",
    "class_1_over = class_1.sample(class_count_0, replace=True)\n",
    "\n",
    "test_over = pd.concat([class_1_over, class_0], axis=0)\n",
    "\n",
    "print(\"Total de clase 1 y 0:\",test_over['Class'].value_counts())#grafica la cuenta después del sobremuestreo\n",
    "test_over['Class'].value_counts().plot(kind='bar', title='Cuenta (Class)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Balancear datos con la librería \"imblearn\"\n",
    "\n",
    "Se han propuesto varias técnicas de remuestreo más sofisticadas.\n",
    "\n",
    "Por ejemplo, para el **submuestreo** podemos agrupar los registros de la clase mayoritaria y realizar el submuestreo eliminando registros de cada grupo, buscando así preservar la información. En el **sobremuestreo**, en lugar de crear copias exactas de los registros la clase minoritaria, podemos introducir pequeñas variaciones en esas copias, creando muestras sintéticas más diversas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install delayed\n",
    "#pip install imblearn\n",
    "#pip install collections\n",
    "import delayed\n",
    "import imblearn\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submuestreo\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42, replacement=True)\n",
    "x_rus, y_rus = rus.fit_resample(x, y)\n",
    "\n",
    "print('Dimension de los datos originales:', Counter(y))\n",
    "print('Dimension de los datos submuestreados', Counter(y_rus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sobremuestreo\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "x_ros, y_ros = ros.fit_resample(x, y)\n",
    "\n",
    "print('Dimension de los datos originales:', Counter(y))\n",
    "print('Dimension de los datos sobremuestreados', Counter(y_ros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submuestreo Tomek links\n",
    "from IPython.display import Image\n",
    "Image(filename='Tomek.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "tl = TomekLinks(sampling_strategy='majority')\n",
    "\n",
    "#ajustando\n",
    "x_tl, y_tl = tl.fit_resample(x.iloc[:1000,:], y.iloc[:1000])\n",
    "\n",
    "\n",
    "print('Dimension de los datos originales:', Counter(y.iloc[:1000]))\n",
    "print('Dimension de los datos submuestreados', Counter(y_tl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sobremuestreo Synthetic Minority Oversampling Technique (SMOTE)\n",
    "from IPython.display import Image\n",
    "Image(filename='SMOTE.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "\n",
    "#Ajustando\n",
    "x_smote, y_smote = smote.fit_resample(x, y)\n",
    "\n",
    "print('Dimension de los datos originales:', Counter(y))\n",
    "print('Dimension de los datos submuestreados', Counter(y_smote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ChessUrl](https://miro.medium.com/max/400/1*z6-XWtRlCDKciBd3eS4PzQ.gif \"end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Sara E. Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
