{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://dinhanhthi.com/img/post/ML/random-forest-decision-tree/r2.jpg\" width=\"350px\" height=\"180px\" />\n",
    "\n",
    "\n",
    "# <font color= #8A0829> Laboratorio de Modelado de Datos </font>\n",
    "- <Strong> Flavio Maximiliano Herrada Avalos</Strong>\n",
    "- <Strong> Año </Strong>: 2021\n",
    "- <Strong> Email: </Strong>  <font color=\"blue\"> `flavio.herrada@iteso.mx` </font>\n",
    "___\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://dinhanhthi.com/img/post/ML/random-forest-decision-tree/r2.jpg</p>\n",
    "\n",
    "### <font color= #2E9AFE> Tema: Modelos basados en Árboles</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un árbol de decisión es un conjunto de sentencias de la forma: si... entonces...\n",
    "\n",
    "Estas sentencias dividen los datos en una serie de predictores. \n",
    "\n",
    "Estas divisiones de predictores son usadas para estimar la salida del modelo. \n",
    "\n",
    "**Ejemplo:** En un problema con dos predictores 'A' y 'B', un conjunto de sentencias de la forma:\n",
    "\n",
    "Si el predictor 'A' >= 1.7, entonces\n",
    "- Si el predictor B >=200, entonces la predicción = 1.3\n",
    "- De otra forma, la predicción = 2.5\n",
    "\n",
    "Estas sentencias están partiendo los predictores en 3 segmentos donde salidas específicas son definidas. \n",
    "\n",
    "**Estructura del árbol**\n",
    "\n",
    "Toma 3 cosas en cuenta:\n",
    "- Las variables predictoras (X) que se van a usar y el punto de partición del dataset.\n",
    "- La profundidad/complejidad del árbol\n",
    "- La ecuación de predicción en los últimos nodos/hojas del árbol\n",
    "\n",
    "**Hiperparámetros a ajustar**\n",
    "- Profundidad del árbol (max_depth)\n",
    "- Número mínimo de observaciones en cada split(min_samples_split)\n",
    "\n",
    "**Desventajas**\n",
    "\n",
    "- Inestabilidad del modelo: Debido a que las particiones se basan en un conjunto de datos, si se generan cambios en el conjunto de datos, esto genera cambios importantes en la estructura del árbol y especialmente en su interpretabilidad.\n",
    "\n",
    "- Rendimiento predictivo subóptimo. Nuevamente, debido a que las particiones se basan en un conjunto de datos específico, el modelo generalmente no converge con el modelo óptimo global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generación de datos\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# #############################################################################\n",
    "X = np.zeros((100,3))\n",
    "X[:,0] = 5 * rng.rand(100, 1)[:,0]\n",
    "X[:,1] = 10 * rng.rand(100, 1)[:,0]\n",
    "X[:,2] = 15 * rng.rand(100, 1)[:,0]\n",
    "\n",
    "y = np.ravel(3*X[:,0]-2*X[:,1]+3*X[:,2]-10)\n",
    "# Añadimos ruido a la variable dependiente\n",
    "yrnd = y + 3 * (0.5 - rng.rand(y.shape[0]))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "plt.scatter(X[:,0], yrnd, c='r', s=10)\n",
    "plt.xlabel('x_p1')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(X[:,1], yrnd, c='r', s=10,zorder=2)\n",
    "plt.xlabel('x_p2')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(X[:,2], yrnd, c='r', s=10, zorder=2)\n",
    "plt.xlabel('x_p3')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, yrnd, test_size=0.20,\n",
    "                                                    random_state=0,\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "max_depths = range(1, 20)\n",
    "\n",
    "#para todos los datos\n",
    "training_error = []\n",
    "for max_depth in max_depths:\n",
    "    model_1 = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    model_1.fit(X, yrnd)\n",
    "    training_error.append(mse(yrnd, model_1.predict(X)))\n",
    "    \n",
    "\n",
    "#para el train-test    \n",
    "testing_error = []\n",
    "for max_depth in max_depths:\n",
    "    model_2 = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    model_2.fit(X_train, y_train)\n",
    "    testing_error.append(mse(y_test, model_2.predict(X_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cómo saber el número óptimo de profundidad?\n",
    "#Una forma es graficando\n",
    "plt.plot(max_depths, training_error, color='blue', label='Training error')\n",
    "plt.plot(max_depths, testing_error, color='green', label='Testing error')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.axvline(x=6, color='orange', linestyle='--')\n",
    "plt.annotate('optimum = 6', xy=(7.5, 1.17), color='red')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.title('Hyperparameter Tuning', pad=15, size=15)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usando cross validation y grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "gs = GridSearchCV(model,\n",
    "                  param_grid = {'max_depth': range(1, 11),\n",
    "                                'min_samples_split': range(10, 60, 10)},\n",
    "                  cv=5,\n",
    "                  scoring='neg_mean_squared_error')\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crear modelo usando parámetros óptimos\n",
    "new_model = DecisionTreeRegressor(max_depth=6,\n",
    "                                  min_samples_split=10)\n",
    "new_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "yhat = new_model.predict(X_test)\n",
    "R2_score = r2_score(y_test,yhat)\n",
    "print('R2:', R2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "MSE_score = mean_squared_error(y_test,yhat)\n",
    "print('MSE:', MSE_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ver importancia de las variables\n",
    "new_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "tree.plot_tree(new_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correr en jupyter notebook:\n",
    "#pip install graphviz\n",
    "\n",
    "#correr en la consola de anaconda\n",
    "#conda install python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "dot_data = tree.export_graphviz(new_model, out_file=None)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"trees_gen/tree_multipredict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ventajas de los árboles de decisión**\n",
    "\n",
    "- No requiere escalamiento de variables\n",
    "- Puede  ser usado para datos no lineales\n",
    "- Fácil de visualizar\n",
    "- Fácil de interpretar\n",
    "\n",
    "**Desventajas de los árboles de decisión**\n",
    "\n",
    "- Es computancionalmente complejo, especialmente al usar cross-validation para ajustar los hiperparámetros\n",
    "- Un cambio pequeño en los datos puede causar grandes cambios en la estructura del árbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
